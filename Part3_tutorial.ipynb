{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f0b9bb",
   "metadata": {},
   "source": [
    "# Part 3: Using nnAudio Trainable Basis Functions with others model\n",
    "\n",
    "In part 1 & 2 tutorial, we used linear model as an example to show the effect of nnAudio trainable basis functions.\\\n",
    "In this tutorial, `Broadcasting-residual network (BC_ResNet)` will be used for demonstration on how nnAudio is applied in others model. You can simply `replace the code in Step 8` with others model.\n",
    "\n",
    "[Step 1: import related libraries](#Step-1:-import-related-libraries)\\\n",
    "[Step 2: setting up configuration](#Step-2:-setting-up-configuration)\\\n",
    "[Step 3: setting up nnAudio basis functions](#Step-3:-setting-up-nnAudio-basis-functions)\\\n",
    "[Step 4: setting up dataset](#Step-4:-setting-up-dataset)\\\n",
    "[Step 5: data rebalancing](#Step-5:-data-rebalancing)\\\n",
    "[Step 6: data processing and loading](#Step-6:-data-processing-and-loading)\\\n",
    "[Step 7: setting up the ligthning module](#Step-7:-setting-up-the-ligthning-module)\\\n",
    "[Step 8: setting up model](#Step-8:-setting-up-model)\\\n",
    "[Step 9: training model](#Step-9:-training-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b55681",
   "metadata": {},
   "source": [
    "## Step 1: import related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries related to PyTorch\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import WeightedRandomSampler,DataLoader\n",
    "import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# Libraries related to PyTorch Lightning\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "\n",
    "# Libraries used in ligthning module\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "# Libraried related to dataset\n",
    "from AudioLoader.Speech import SPEECHCOMMANDS_12C #for 12 classes KWS task\n",
    "\n",
    "# nnAudio Front-end\n",
    "from nnAudio.features.mel import MelSpectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc46fca0",
   "metadata": {},
   "source": [
    "## Step 2: setting up configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe6b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "gpus = 1\n",
    "batch_size= 100\n",
    "max_epochs = 200\n",
    "check_val_every_n_epoch = 2\n",
    "num_sanity_val_steps = 5\n",
    "\n",
    "data_root= './' # Download the data here\n",
    "download_option= False\n",
    "\n",
    "n_mels= 40 \n",
    "#number of Mel bases\n",
    "\n",
    "output_dim= 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07de7ae",
   "metadata": {},
   "source": [
    "# nnAudio guideline for trainable basis functions \n",
    "```\n",
    "nnAudio.features.mel.MelSpectrogram(trainable_mel= ,trainable_STFT=) \n",
    "```\n",
    "The function above is controlling if mel bases and STFT trainable\n",
    "\n",
    "* A. Both Mel and STFT are non-trainable: \n",
    "`trainable_mel=False, trainable_STFT=False`\n",
    "* B. Mel is trainable while STFT is fixed: \n",
    "`trainable_mel=True, trainable_STFT=False`\n",
    "* C. Mel is fixed while STFT is trainable: \n",
    "`trainable_mel=False, trainable_STFT=True`\n",
    "* D. Both Mel and STFT are trainable:\n",
    "`trainable_mel=True, trainable_STFT=True`\n",
    "\n",
    "## Step 3: setting up nnAudio basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b8cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_layer = MelSpectrogram(sr=16000, \n",
    "                           n_fft=480,\n",
    "                           win_length=None,\n",
    "                           n_mels=n_mels, \n",
    "                           hop_length=160,\n",
    "                           window='hann',\n",
    "                           center=True,\n",
    "                           pad_mode='reflect',\n",
    "                           power=2.0,\n",
    "                           htk=False,\n",
    "                           fmin=0.0,\n",
    "                           fmax=None,\n",
    "                           norm=1,\n",
    "                           trainable_mel=True,\n",
    "                           trainable_STFT=False,\n",
    "                           verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484cb3e7",
   "metadata": {},
   "source": [
    "## Step 4: setting up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e3e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = SPEECHCOMMANDS_12C(root=data_root,\n",
    "                              url='speech_commands_v0.02',\n",
    "                              folder_in_archive='SpeechCommands',\n",
    "                              download= download_option,subset= 'training') \n",
    "\n",
    "validset = SPEECHCOMMANDS_12C(root=data_root,\n",
    "                              url='speech_commands_v0.02',\n",
    "                              folder_in_archive='SpeechCommands',\n",
    "                              download= download_option,subset= 'validation')\n",
    "\n",
    "testset = SPEECHCOMMANDS_12C(root=data_root,\n",
    "                              url='speech_commands_v0.02',\n",
    "                              folder_in_archive='SpeechCommands',\n",
    "                              download= download_option,subset= 'testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c66cc52",
   "metadata": {},
   "source": [
    "## Step 5: data rebalancing\n",
    "\n",
    "For class weighting, rebalancing silence(10th class) and unknown(11th class) in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cabc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = [1,1,1,1,1,1,1,1,1,1,4.6,1/17]\n",
    "sample_weights = [0] * len(trainset)\n",
    "#create a list as per length of trainset\n",
    "\n",
    "for idx, (data,rate,label,speaker_id, _) in enumerate(trainset):\n",
    "    class_weight = class_weights[label]\n",
    "    sample_weights[idx] = class_weight\n",
    "#apply sample_weights in each data base on their label class in class_weight\n",
    "\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights),replacement=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ed8c7a",
   "metadata": {},
   "source": [
    "## Step 6: data processing and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5510dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processing\n",
    "def data_processing(data):\n",
    "    waveforms = []\n",
    "    labels = []\n",
    "    \n",
    "    for batch in data:\n",
    "        waveforms.append(batch[0].squeeze(0)) #after squeeze => (audio_len) tensor # remove batch dim\n",
    "        labels.append(batch[2])      \n",
    "        \n",
    "    waveform_padded = nn.utils.rnn.pad_sequence(waveforms, batch_first=True)  \n",
    "    \n",
    "    output_batch = {'waveforms': waveform_padded, \n",
    "             'labels': torch.tensor(labels),\n",
    "             }\n",
    "    return output_batch\n",
    "\n",
    "#load data\n",
    "trainloader = DataLoader(trainset,                                \n",
    "                              collate_fn=lambda x: data_processing(x),\n",
    "                                         batch_size=batch_size,sampler=sampler,num_workers=1)\n",
    "\n",
    "validloader = DataLoader(validset,                               \n",
    "                              collate_fn=lambda x: data_processing(x),\n",
    "                                         batch_size=batch_size,num_workers=1)\n",
    "\n",
    "testloader = DataLoader(testset,   \n",
    "                              collate_fn=lambda x: data_processing(x),\n",
    "                                        batch_size=batch_size,num_workers=1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2061eec4",
   "metadata": {},
   "source": [
    "## Step 7: setting up the ligthning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db981f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommand(LightningModule):\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs, spec = self(batch['waveforms']) \n",
    "        #return outputs [2D] for calculate loss, return spec [3D] for visual\n",
    "        loss = self.criterion(outputs, batch['labels'].long())\n",
    "\n",
    "        acc = sum(outputs.argmax(-1) == batch['labels'])/outputs.shape[0] #batch wise\n",
    "        \n",
    "        self.log('Train/acc', acc, on_step=False, on_epoch=True)\n",
    "          \n",
    "        self.log('Train/Loss', loss, on_step=False, on_epoch=True)\n",
    "        #log(graph title, take acc as data, on_step: plot every step, on_epch: plot every epoch)\n",
    "        return loss\n",
    "\n",
    "     \n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx,\n",
    "                       optimizer_closure, on_tpu, using_native_amp, using_lbfgs):\n",
    "        \n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "        with torch.no_grad():\n",
    "            torch.clamp_(self.mel_layer.mel_basis, 0, 1)\n",
    "        #after optimizer step, do clamp function on mel_basis \n",
    "        \n",
    "   \n",
    "    def validation_step(self, batch, batch_idx):               \n",
    "        outputs, spec = self(batch['waveforms'])\n",
    "        loss = self.criterion(outputs, batch['labels'].long())        \n",
    "       \n",
    "        self.log('Validation/Loss', loss, on_step=False, on_epoch=True)          \n",
    "\n",
    "        output_dict = {'outputs': outputs,\n",
    "                       'labels': batch['labels']}        \n",
    "        return output_dict\n",
    "\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        pred = []\n",
    "        label = []\n",
    "        for output in outputs:\n",
    "            pred.append(output['outputs'])\n",
    "            label.append(output['labels'])\n",
    "        label = torch.cat(label, 0)\n",
    "        pred = torch.cat(pred, 0)\n",
    "        acc = sum(pred.argmax(-1) == label)/label.shape[0]\n",
    "        \n",
    "        self.log('Validation/acc', acc, on_step=False, on_epoch=True)    \n",
    "        #use the return value from validation_step: output_dict , to calculate the overall accuracy   \n",
    "        #epoch wise \n",
    "                              \n",
    "    def test_step(self, batch, batch_idx):               \n",
    "        outputs, spec = self(batch['waveforms'])\n",
    "        loss = self.criterion(outputs, batch['labels'].long())        \n",
    "\n",
    "        self.log('Test/Loss', loss, on_step=False, on_epoch=True)          \n",
    "\n",
    "        output_dict = {'outputs': outputs,\n",
    "                       'labels': batch['labels']}        \n",
    "        return output_dict\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        pred = []\n",
    "        label = []\n",
    "        for output in outputs:\n",
    "            pred.append(output['outputs'])\n",
    "            label.append(output['labels'])\n",
    "        label = torch.cat(label, 0)\n",
    "        pred = torch.cat(pred, 0)\n",
    "        \n",
    "        result_dict = {}\n",
    "        for key in [None, 'micro', 'macro', 'weighted']:\n",
    "            result_dict[key] = {}\n",
    "            p, r, f1, _ = precision_recall_fscore_support(label.cpu(), pred.argmax(-1).cpu(), average=key, zero_division=0)\n",
    "            result_dict[key]['precision'] = p\n",
    "            result_dict[key]['recall'] = r\n",
    "            result_dict[key]['f1'] = f1\n",
    "            \n",
    "        acc = sum(pred.argmax(-1) == label)/label.shape[0]\n",
    "        self.log('Test/acc', acc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        self.log('Test/micro_f1', result_dict['micro']['f1'], on_step=False, on_epoch=True)\n",
    "        self.log('Test/macro_f1', result_dict['macro']['f1'], on_step=False, on_epoch=True)\n",
    "        self.log('Test/weighted_f1', result_dict['weighted']['f1'], on_step=False, on_epoch=True)\n",
    "        \n",
    "        torch.save(result_dict, \"result_dict.pt\")        \n",
    "        \n",
    "        return result_dict        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        model_param = []\n",
    "        for name, params in self.named_parameters():\n",
    "            if 'mel_layer.' in name:\n",
    "                pass\n",
    "            else:\n",
    "                model_param.append(params)          \n",
    "        optimizer = optim.SGD([\n",
    "                                {\"params\": self.mel_layer.parameters(),\n",
    "                                 \"lr\": 1e-3,\n",
    "                                 \"momentum\": 0.9,\n",
    "                                 \"weight_decay\": 0.001},\n",
    "                                {\"params\": model_param,\n",
    "                                 \"lr\": 1e-3,\n",
    "                                 \"momentum\": 0.9,\n",
    "                                 \"weight_decay\": 0.001}            \n",
    "                              ])\n",
    "        #for applying diff lr in model and mel bases function       \n",
    "        return [optimizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9dcef1",
   "metadata": {},
   "source": [
    "## Step 8: setting up model \n",
    "You can replace the code below with others model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2cd87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BC_ResNet model\n",
    "class SubSpectralNorm(LightningModule):\n",
    "    def __init__(self, C, S, eps=1e-5):\n",
    "        super(SubSpectralNorm, self).__init__()\n",
    "        self.S = S\n",
    "        self.eps = eps\n",
    "        self.bn = nn.BatchNorm2d(C*S)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input features with shape {N, C, F, T}\n",
    "        # S: number of sub-bands\n",
    "        N, C, F, T = x.size()\n",
    "        x = x.view(N, C * self.S, F // self.S, T)\n",
    "\n",
    "        x = self.bn(x)\n",
    "        return x.view(N, C, F, T)\n",
    "    \n",
    "class BroadcastedBlock(LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            planes: int,\n",
    "            dilation=1,\n",
    "            stride=1,\n",
    "            temp_pad=(0, 1),\n",
    "    ) -> None:\n",
    "        super(BroadcastedBlock, self).__init__()\n",
    "\n",
    "        self.freq_dw_conv = nn.Conv2d(planes, planes, kernel_size=(3, 1), padding=(1, 0), groups=planes,\n",
    "                                      dilation=dilation,\n",
    "                                      stride=stride, bias=False)\n",
    "        self.ssn1 = SubSpectralNorm(planes, 5)\n",
    "        self.temp_dw_conv = nn.Conv2d(planes, planes, kernel_size=(1, 3), padding=temp_pad, groups=planes,\n",
    "                                      dilation=dilation, stride=stride, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.channel_drop = nn.Dropout2d(p=0.1)\n",
    "        self.swish = nn.SiLU()\n",
    "        self.conv1x1 = nn.Conv2d(planes, planes, kernel_size=(1, 1), bias=False)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        # f2\n",
    "        ##########################\n",
    "        out = self.freq_dw_conv(x)\n",
    "        out = self.ssn1(out)\n",
    "        ##########################\n",
    "\n",
    "        auxilary = out\n",
    "        out = out.mean(2, keepdim=True)  # frequency average pooling\n",
    "\n",
    "        # f1\n",
    "        ############################\n",
    "        out = self.temp_dw_conv(out)\n",
    "        out = self.bn(out)\n",
    "        out = self.swish(out)\n",
    "        out = self.conv1x1(out)\n",
    "        out = self.channel_drop(out)\n",
    "        ############################\n",
    "\n",
    "        out = out + identity + auxilary\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class TransitionBlock(LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            inplanes: int,\n",
    "            planes: int,\n",
    "            dilation=1,\n",
    "            stride=1,\n",
    "            temp_pad=(0, 1),\n",
    "    ) -> None:\n",
    "        super(TransitionBlock, self).__init__()\n",
    "\n",
    "        self.freq_dw_conv = nn.Conv2d(planes, planes, kernel_size=(3, 1), padding=(1, 0), groups=planes,\n",
    "                                      stride=stride,\n",
    "                                      dilation=dilation, bias=False)\n",
    "        self.ssn = SubSpectralNorm(planes, 5)\n",
    "        self.temp_dw_conv = nn.Conv2d(planes, planes, kernel_size=(1, 3), padding=temp_pad, groups=planes,\n",
    "                                      dilation=dilation, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.channel_drop = nn.Dropout2d(p=0.5)\n",
    "        self.swish = nn.SiLU()\n",
    "        self.conv1x1_1 = nn.Conv2d(inplanes, planes, kernel_size=(1, 1), bias=False)\n",
    "        self.conv1x1_2 = nn.Conv2d(planes, planes, kernel_size=(1, 1), bias=False)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # f2\n",
    "        #############################\n",
    "        out = self.conv1x1_1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.freq_dw_conv(out)\n",
    "        out = self.ssn(out)\n",
    "        #############################\n",
    "        auxilary = out\n",
    "        out = out.mean(2, keepdim=True)  # frequency average pooling\n",
    "\n",
    "        # f1\n",
    "        #############################\n",
    "        out = self.temp_dw_conv(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.swish(out)\n",
    "        out = self.conv1x1_2(out)\n",
    "        out = self.channel_drop(out)\n",
    "        #############################\n",
    "\n",
    "        out = auxilary + out\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    \n",
    "class BCResNet_nnAudio(SpeechCommand):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5, stride=(2, 1), padding=(2, 2))\n",
    "        self.block1_1 = TransitionBlock(16, 8)\n",
    "        self.block1_2 = BroadcastedBlock(8)\n",
    "\n",
    "        self.block2_1 = TransitionBlock(8, 12, stride=(2, 1), dilation=(1, 2), temp_pad=(0, 2))\n",
    "        self.block2_2 = BroadcastedBlock(12, dilation=(1, 2), temp_pad=(0, 2))\n",
    "\n",
    "        self.block3_1 = TransitionBlock(12, 16, stride=(2, 1), dilation=(1, 4), temp_pad=(0, 4))\n",
    "        self.block3_2 = BroadcastedBlock(16, dilation=(1, 4), temp_pad=(0, 4))\n",
    "        self.block3_3 = BroadcastedBlock(16, dilation=(1, 4), temp_pad=(0, 4))\n",
    "        self.block3_4 = BroadcastedBlock(16, dilation=(1, 4), temp_pad=(0, 4))\n",
    "\n",
    "        self.block4_1 = TransitionBlock(16, 20, dilation=(1, 8), temp_pad=(0, 8))\n",
    "        self.block4_2 = BroadcastedBlock(20, dilation=(1, 8), temp_pad=(0, 8))\n",
    "        self.block4_3 = BroadcastedBlock(20, dilation=(1, 8), temp_pad=(0, 8))\n",
    "        self.block4_4 = BroadcastedBlock(20, dilation=(1, 8), temp_pad=(0, 8))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(20, 20, 5, groups=20, padding=(0, 2))\n",
    "        self.conv3 = nn.Conv2d(20, 32, 1, bias=False)\n",
    "        self.conv4 = nn.Conv2d(32, output_dim, 1, bias=False)\n",
    "                \n",
    "        self.mel_layer = mel_layer \n",
    "        self.criterion = nn.CrossEntropyLoss()        \n",
    "     \n",
    "\n",
    "    def forward(self, x):        \n",
    "        #x: 2D [Batch_size,16000]\n",
    "        spec = self.mel_layer(x) \n",
    "        #spec: 3D [B,F(40),T]\n",
    "        \n",
    "        spec = torch.log(spec+1e-10)         \n",
    "        spec = spec.unsqueeze(1)\n",
    "        #spec: bcoz conv1 need 4D [B,1,F,T]\n",
    "\n",
    "        out = self.conv1(spec)\n",
    "        out = self.block1_1(out)\n",
    "        out = self.block1_2(out)\n",
    "\n",
    "        out = self.block2_1(out)\n",
    "        out = self.block2_2(out)\n",
    "\n",
    "        out = self.block3_1(out)\n",
    "        out = self.block3_2(out)\n",
    "        out = self.block3_3(out)\n",
    "        out = self.block3_4(out)\n",
    "\n",
    "        out = self.block4_1(out)\n",
    "        out = self.block4_2(out)\n",
    "        out = self.block4_3(out)\n",
    "        out = self.block4_4(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = out.mean(-1, keepdim=True)\n",
    "\n",
    "        out = self.conv4(out)   \n",
    "        #out: 4D [8, 35, 1, 1]\n",
    "        out = out.squeeze(2).squeeze(2)  \n",
    "        #out: 2D        \n",
    "        #crossentropy expect[B, C], so need to squeeze to be 2D\n",
    "\n",
    "        spec = spec.squeeze(1) \n",
    "        #spec: from 4D [B,1,F,T] to 3D [B,F,T]\n",
    "        #the return spec is for plot log_images, so need 3D\n",
    "\n",
    "        return out, spec\n",
    "    \n",
    "net = BCResNet_nnAudio()\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a885e",
   "metadata": {},
   "source": [
    "## Step 9: training model\n",
    "\n",
    "Everytime you train a model, the trained weight will be saved in `./lightning_logs/version_<XX>` folder.\\\n",
    "You can use `Part2_tutorial` to visualize the result.\n",
    "\n",
    "Follow the instructions below to visualize your model performance:\n",
    "1. Simply replace `linear model (Step 7 in part 2 tutorial)` with `your model (step 8 in part 3 tutorail)`.\n",
    "1. Amend the configuration part (Step 2 in part 2 tutorial) if needed.\n",
    "1. Load your trained model weight (Step 8 & Visualizing result section in part 2 tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(gpus=gpus, max_epochs=max_epochs,\n",
    "    check_val_every_n_epoch= check_val_every_n_epoch,\n",
    "    num_sanity_val_steps=num_sanity_val_steps)\n",
    "\n",
    "trainer.fit(net, trainloader, validloader)\n",
    "trainer.test(net, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4f7107",
   "metadata": {},
   "source": [
    "# This is the end of nnAudio tutorial. \n",
    "# Thank you for using ~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8.10",
   "language": "python",
   "name": "python3.8.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
