{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ac60b08",
   "metadata": {},
   "source": [
    "# Part 2: Evaluation with Trainable Basis Functions\n",
    "\n",
    "[Step 1: import related libraries](#Step-1:-import-related-libraries)\\\n",
    "[Step 2: setting up configuration](#Step-2:-setting-up-configuration)\\\n",
    "[Step 3: setting up nnAudio basis functions](#Step-3:-setting-up-nnAudio-basis-functions)\\\n",
    "[Step 4: setting up dataset](#Step-4:-setting-up-dataset)\\\n",
    "[Step 5: data processing and loading](#Step-5:-data-processing-and-loading)\\\n",
    "[Step 6: setting up the ligthning module](#Step-6:-setting-up-the-ligthning-module)\\\n",
    "[Step 7: setting up model and applying the ligthning module](#Step-7:-setting-up-model-and-applying-the-ligthning-module)\\\n",
    "[Step 8: loading pre-trained weight to model](#Step-8:-loading-pre-trained-weight-to-model)\\\n",
    "[Step 9: evaluating the model performance](#Step-9:-evaluating-the-model-performance)\n",
    "\n",
    "[Visualizing result](#Visualizing-result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2655df1",
   "metadata": {},
   "source": [
    "## Step 1: import related libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b48aa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries related to PyTorch\n",
    "import torch\n",
    "from torch import Tensor \n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Libraries related to PyTorch Lightning\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "\n",
    "# Libraries used for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "#Libraries related to dataset\n",
    "from AudioLoader.Speech import SPEECHCOMMANDS_12C #for 12 classes KWS task\n",
    "from AudioLoader.Speech import idx2name, name2idx\n",
    "from collections import OrderedDict\n",
    "\n",
    "# nnAudio Front-end\n",
    "from nnAudio.features.mel import MelSpectrogram, STFT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda8bc20",
   "metadata": {},
   "source": [
    "## Step 2: setting up configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "gpus = 1\n",
    "batch_size= 100\n",
    "max_epochs = 200\n",
    "check_val_every_n_epoch = 2\n",
    "num_sanity_val_steps = 5\n",
    "\n",
    "data_root= './' # Download the data here\n",
    "download_option= False\n",
    "\n",
    "n_mels= 40 \n",
    "#number of Mel bases\n",
    "\n",
    "input_dim= (n_mels*101)\n",
    "output_dim= 12\n",
    "\n",
    "random_mel= False   \n",
    "#To control random initial mel bases  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c765503a",
   "metadata": {},
   "source": [
    "# nnAudio guideline for trainable basis functions \n",
    "```\n",
    "nnAudio.features.mel.MelSpectrogram(trainable_mel= ,trainable_STFT=) \n",
    "```\n",
    "The function above is controlling if mel bases and STFT trainable\n",
    "\n",
    "* A. Both Mel and STFT are non-trainable: \n",
    "`trainable_mel=False, trainable_STFT=False`\n",
    "* B. Mel is trainable while STFT is fixed: \n",
    "`trainable_mel=True, trainable_STFT=False`\n",
    "* C. Mel is fixed while STFT is trainable: \n",
    "`trainable_mel=False, trainable_STFT=True`\n",
    "* D. Both Mel and STFT are trainable:\n",
    "`trainable_mel=True, trainable_STFT=True`\n",
    "\n",
    "## Step 3: setting up nnAudio basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab9ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_layer = MelSpectrogram(sr=16000, \n",
    "                           n_fft=480,\n",
    "                           win_length=None,\n",
    "                           n_mels=n_mels, \n",
    "                           hop_length=160,\n",
    "                           window='hann',\n",
    "                           center=True,\n",
    "                           pad_mode='reflect',\n",
    "                           power=2.0,\n",
    "                           htk=False,\n",
    "                           fmin=0.0,\n",
    "                           fmax=None,\n",
    "                           norm=1,\n",
    "                           trainable_mel=True,\n",
    "                           trainable_STFT=False,\n",
    "                           verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3789b5b7",
   "metadata": {},
   "source": [
    "## Step 4: setting up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33040bfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testset = SPEECHCOMMANDS_12C(root=data_root,\n",
    "                              url='speech_commands_v0.02',\n",
    "                              folder_in_archive='SpeechCommands',\n",
    "                              download= download_option,subset= 'testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd65a8",
   "metadata": {},
   "source": [
    "## Step 5: data processing and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processing\n",
    "def data_processing(data):\n",
    "    waveforms = []\n",
    "    labels = []\n",
    "    \n",
    "    for batch in data:\n",
    "        waveforms.append(batch[0].squeeze(0)) #after squeeze => (audio_len) tensor # remove batch dim\n",
    "        labels.append(batch[2])      \n",
    "        \n",
    "    waveform_padded = nn.utils.rnn.pad_sequence(waveforms, batch_first=True)  \n",
    "    \n",
    "    output_batch = {'waveforms': waveform_padded, \n",
    "             'labels': torch.tensor(labels),\n",
    "             }\n",
    "    return output_batch\n",
    "\n",
    "#load data\n",
    "testloader = DataLoader(testset,   \n",
    "                              collate_fn=lambda x: data_processing(x),\n",
    "                                        batch_size=batch_size, num_workers =1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f203f680",
   "metadata": {},
   "source": [
    "## Step 6: setting up the ligthning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906b0fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommand(LightningModule):     \n",
    "    def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx,\n",
    "                       optimizer_closure, on_tpu, using_native_amp, using_lbfgs):\n",
    "        \n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "        with torch.no_grad():\n",
    "            torch.clamp_(self.mel_layer.mel_basis, 0, 1)\n",
    "        #after optimizer step, do clamp function on mel_basis\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):               \n",
    "        outputs, spec = self(batch['waveforms'])\n",
    "        loss = self.criterion(outputs, batch['labels'].long())        \n",
    "\n",
    "        self.log('Test/Loss', loss, on_step=False, on_epoch=True)          \n",
    "        \n",
    "        output_dict = {'outputs': outputs,\n",
    "                       'labels': batch['labels']}        \n",
    "        return output_dict\n",
    "    \n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        pred = []\n",
    "        label = []\n",
    "        for output in outputs:\n",
    "            pred.append(output['outputs'])\n",
    "            label.append(output['labels'])\n",
    "        label = torch.cat(label, 0)\n",
    "        pred = torch.cat(pred, 0)\n",
    "        \n",
    "        result_dict = {}\n",
    "        for key in [None, 'micro', 'macro', 'weighted']:\n",
    "            result_dict[key] = {}\n",
    "            p, r, f1, _ = precision_recall_fscore_support(label.cpu(), pred.argmax(-1).cpu(), average=key, zero_division=0)\n",
    "            result_dict[key]['precision'] = p\n",
    "            result_dict[key]['recall'] = r\n",
    "            result_dict[key]['f1'] = f1\n",
    "            \n",
    "        barplot(result_dict, 'precision', figsize=(4,6))\n",
    "        barplot(result_dict, 'recall',figsize=(4,6))\n",
    "        barplot(result_dict, 'f1',figsize=(4,6))\n",
    "            \n",
    "        acc = sum(pred.argmax(-1) == label)/label.shape[0]\n",
    "        self.log('Test/acc', acc, on_step=False, on_epoch=True)\n",
    "        \n",
    "        self.log('Test/micro_f1', result_dict['micro']['f1'], on_step=False, on_epoch=True)\n",
    "        self.log('Test/macro_f1', result_dict['macro']['f1'], on_step=False, on_epoch=True)\n",
    "        self.log('Test/weighted_f1', result_dict['weighted']['f1'], on_step=False, on_epoch=True)\n",
    "        \n",
    "        cm = plot_confusion_matrix(label.cpu(),\n",
    "                                   pred.argmax(-1).cpu(),\n",
    "                                   name2idx.keys(),\n",
    "                                   title='Test: Confusion matrix',\n",
    "                                   normalize=False)                    \n",
    "        return result_dict\n",
    "\n",
    "    \n",
    "def plot_confusion_matrix(correct_labels,\n",
    "                          predict_labels,\n",
    "                          labels,\n",
    "                          title='Confusion matrix',\n",
    "                          normalize=False):\n",
    "    ''' \n",
    "    Parameters:\n",
    "        correct_labels                  : These are your true classification categories.\n",
    "        predict_labels                  : These are you predicted classification categories\n",
    "        labels                          : This is a lit of labels which will be used to display the axix labels\n",
    "        title='Confusion matrix'        : Title for your matrix\n",
    "        tensor_name = 'MyFigure/image'  : Name for the output summay tensor\n",
    "    Returns:\n",
    "        summary: TensorFlow summary \n",
    "    Other itema to note:\n",
    "        - Depending on the number of category and the data , you may have to modify the figzie, font sizes etc. \n",
    "        - Currently, some of the ticks dont line up due to rotations.\n",
    "    '''\n",
    "    cm = confusion_matrix(correct_labels, predict_labels, labels=range(len(labels)))\n",
    "    if normalize:\n",
    "        cm = cm.astype('float')*10 / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm = np.nan_to_num(cm, copy=True)\n",
    "        cm = cm.astype('int')\n",
    "\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4.5, 4.5), dpi=160, facecolor='w', edgecolor='k')\n",
    "    fig.suptitle('confusion_matrix',fontsize=7)\n",
    "    im = ax.imshow(cm, cmap='Oranges')\n",
    "\n",
    "    classes = [re.sub(r'([a-z](?=[A-Z])|[A-Z](?=[A-Z][a-z]))', r'\\1 ', x) for x in labels]\n",
    "    #classes = ['\\n'.join(l) for l in classes]\n",
    "\n",
    "    tick_marks = np.arange(len(classes))\n",
    "\n",
    "    ax.set_xlabel('Predicted', fontsize=7)\n",
    "    ax.set_xticks(tick_marks)\n",
    "    c = ax.set_xticklabels(classes, fontsize=5, rotation=0,  ha='center')\n",
    "    ax.xaxis.set_label_position('bottom')\n",
    "    ax.xaxis.tick_bottom()\n",
    "\n",
    "    ax.set_ylabel('True Label', fontsize=7)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_yticklabels(classes, fontsize=5, va ='center')\n",
    "    ax.yaxis.set_label_position('left')\n",
    "    ax.yaxis.tick_left()\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        ax.text(j, i, format(cm[i, j], 'd') if cm[i,j]!=0 else '.', horizontalalignment=\"center\", fontsize=6, verticalalignment='center', color= \"black\")\n",
    "    fig.set_tight_layout(True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "def barplot(result_dict, title, figsize=(4,12), minor_interval=0.2, log=False):\n",
    "    fig, ax = plt.subplots(1,1, figsize=figsize)\n",
    "    metric = {}\n",
    "    for idx, item in enumerate(result_dict[None][title]):\n",
    "        metric[idx2name[idx]] = item\n",
    "    xlabels = list(metric.keys())\n",
    "    values = list(metric.values())\n",
    "    if log:\n",
    "        values = np.log(values)\n",
    "    ax.barh(xlabels, values)\n",
    "    ax.tick_params(labeltop=True, labelright=False)\n",
    "    ax.xaxis.grid(True, which='minor')\n",
    "    ax.xaxis.set_minor_locator(MultipleLocator(minor_interval))\n",
    "    ax.set_ylim([-1,len(xlabels)])\n",
    "    ax.set_title(title)\n",
    "    ax.grid(axis='x')\n",
    "    ax.grid(b=True, which='minor', linestyle='--')\n",
    "    fig.savefig(f'{title}.png', bbox_inches='tight')\n",
    "    fig.tight_layout() # prevent edge from missing\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf02e55",
   "metadata": {},
   "source": [
    "## Step 7: setting up model and applying the ligthning module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cdec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linearmodel_nnAudio(SpeechCommand):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.mel_layer = mel_layer       \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.linearlayer = nn.Linear(input_dim, output_dim)\n",
    "   \n",
    "        if random_mel == True:\n",
    "            nn.init.kaiming_uniform_(self.mel_layer.mel_basis, mode='fan_in')\n",
    "            self.mel_layer.mel_basis.requires_grad = False\n",
    "            torch.relu_(self.mel_layer.mel_basis)\n",
    "            self.mel_layer.mel_basis.requires_grad = True\n",
    "            #for randomly initialize mel bases\n",
    "    \n",
    "    def forward(self, x): \n",
    "        #x: 2D [B, 16000]\n",
    "        spec = self.mel_layer(x)  \n",
    "        #spec: 3D [B, F40, T101]\n",
    "        \n",
    "        spec = torch.log(spec+1e-10)\n",
    "        \n",
    "        flatten_spec = torch.flatten(spec, start_dim=1) \n",
    "        #flatten_spec: 2D [B, F*T(40*101)] \n",
    "        #start_dim: flattening start from 1st dimention\n",
    "        \n",
    "        out = self.linearlayer(flatten_spec) \n",
    "        #out: 2D [B,number of class(12)] \n",
    "                               \n",
    "        return out, spec \n",
    "\n",
    "net = Linearmodel_nnAudio()\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261242f8",
   "metadata": {},
   "source": [
    "## Step 8: loading pre-trained weight to model\n",
    "Everytime you train a model after part 1 tutorial, the trained weight will be saved in `lightning_logs` folder.\\\n",
    "There is a checkpoint file inside `trained_weight_for_tutorial2` folder which can use for demostration in the following.\\\n",
    "The detail of trained weight:\n",
    "* Linearmodel in keyword spotting task\n",
    "* Mel is trainable while STFT is fixed:  `trainable_mel=True, trainable_STFT=False`\n",
    "* `n_mels = 40`\n",
    "* Test/acc = 39.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debd651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weight= net.load_from_checkpoint('trained_weight_for_tutorial2/SGD-n_mels=40-Linearmodel_nnAudio-mel=True-STFT=False-speechcommand/version_1/checkpoints/last.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d01b8a",
   "metadata": {},
   "source": [
    "## Step 9: evaluating the model performance \n",
    "Model performance can be demostrated in the following way:\n",
    "* Test/Loss\n",
    "* Test/acc\n",
    "* F1 matrix \n",
    "* Confusion_matrix\n",
    "\n",
    "**Way to confirm model is trained/weight loaded correctly:**\n",
    "\n",
    "* For Linearmodel in keyword spotting (KWS) task, accuracy for Mel trainable with fixed STFT setting should be around 40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf34eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(gpus=gpus, max_epochs=max_epochs,\n",
    "    check_val_every_n_epoch= check_val_every_n_epoch,\n",
    "    num_sanity_val_steps=num_sanity_val_steps)\n",
    "\n",
    "trainer.test(trained_weight, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6714a219",
   "metadata": {},
   "source": [
    "# Visualizing result \n",
    "We can visualize the effect of trainable basis functions in the following way:\n",
    "1. [Visualizing Mel bases](#Visualizing-Mel-bases)\n",
    "1. [Visualizing short-time Fourier transform (STFT)](#Visualizing-STFT)\n",
    "\n",
    "Before we start visualizing the result, let's explore the structure inside the checkpoint file.\n",
    "```\n",
    "weight=torch.load('xxxx/checkpoints/xxxx.ckpt')\n",
    "├── epoch\n",
    "├── global_step\n",
    "├── pytorch-lightning_version\n",
    "│     \n",
    "├── state_dict\n",
    "│     ├─ mel_layer.mel_basis\n",
    "│     ├─ mel_layer.stft.wsin\n",
    "│     ├─ mel_layer.stft.wcos\n",
    "│     ├─ mel_layer.stft.window_mask   \n",
    "│     ├─ linearlayer.weight\n",
    "│     ├─ linearlayer.bias\n",
    "│     │\n",
    "│     \n",
    "├── callbacks\n",
    "├── optimizer_states\n",
    "├── lr_schedulers\n",
    "```\n",
    "`torch.load('xxxx/checkpoints/xxxx.ckpt')` is a dictionary, its keys can be checked in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae2c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight=torch.load('trained_weight_for_tutorial2/SGD-n_mels=40-Linearmodel_nnAudio-mel=True-STFT=False-speechcommand/version_1/checkpoints/last.ckpt',map_location='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d08cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6604e",
   "metadata": {},
   "source": [
    "`'state_dict'` is one of the dictionary key in the checkpoint file, it is an `OrderedDict` which including the **trained weight for basis functions (Mel bases, STFT) and layer weight (linear layer in this case)**.\\\n",
    "Keys for the 'state_dict' (OrderedDict) can be checked in the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa0fbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99a0a6d",
   "metadata": {},
   "source": [
    "## Visualizing Mel bases\n",
    "Shape of `'mel_layer.mel_basis'` should be `[n_mels, (n_fft/2+1)]`\\\n",
    "In the case of `linear model in keyword spotting task`, shape of 'mel_layer.mel_basis' is `[40, 241]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_filter_banks = weight['state_dict']['mel_layer.mel_basis']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd091a",
   "metadata": {},
   "source": [
    "**Individual Mel bases can be shown in the following:**\n",
    "```python\n",
    "plt.plot(mel_filter_banks[i].cpu().detach().numpy())\n",
    "```\n",
    "Simply replace `i` with `the numebr of Mel base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76943821",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mel_filter_banks[0].cpu().detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d31b6f",
   "metadata": {},
   "source": [
    "**40 Mel bases can be shown in the following:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1d8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in mel_filter_banks:\n",
    "    plt.plot(i.cpu().detach().numpy())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107ffc5",
   "metadata": {},
   "source": [
    "## Visualizing STFT\n",
    "Shape of `'mel_layer.stft.wsin'` and `'mel_layer.stft.wcos'` should be `[(n_fft/2+1),1,n_fft]`\\\n",
    "In the case of `linear model in keyword spotting task`, their shape should be  `[241, 1, 480]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f60bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsin = weight['state_dict']['mel_layer.stft.wsin']\n",
    "wcos = weight['state_dict']['mel_layer.stft.wcos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ec766",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2)\n",
    "for ax, kernel_num in zip(axes.flatten(), [2,10,20,50]):\n",
    "    ax.plot(wsin[kernel_num,0].cpu())\n",
    "    ax.set_ylim(-1,1)\n",
    "    fig.suptitle('sin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c1845",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2)\n",
    "for ax, kernel_num in zip(axes.flatten(), [2,10,20,50]):\n",
    "    ax.plot(wcos[kernel_num,0].cpu())\n",
    "    ax.set_ylim(-1,1)\n",
    "    fig.suptitle('wcos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42204e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_mask = weight['state_dict']['mel_layer.stft.window_mask']\n",
    "#window_mask.shape= [1, 480, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc696f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in window_mask:\n",
    "    plt.plot(i.cpu().detach().numpy())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891467e0",
   "metadata": {},
   "source": [
    "# This is the end of Part2_tutorial.\n",
    "Feel free to move on to `Part 3: Using nnAudio Trainable Basis Functions with others model`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8.10",
   "language": "python",
   "name": "python3.8.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
